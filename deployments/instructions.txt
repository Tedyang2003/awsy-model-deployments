### VLLM Deployment Files
--------------------------------------------
For VLLM deployment, three main components are provided:
- Original Deployment Files
- Unpackaged Helm Chart
- Packaged Helm Chart

These files serve as references for deploying in production or staging environments. Model configurations in `model.json` and the `config.pbtxt` files for each model have been pre-configured according to our Monday discussion.

Below are key deployment considerations for each component.

Secret
-------------------------------------------
serving.kserve.io/s3-endpoint: minio-service.triton-inference-services.svc.cluster.local:9000
serving.kserve.io/s3-usehttps: "0"
serving.kserve.io/s3-region: "us-east-1"
serving.kserve.io/s3-usevirtualbucket: "false"
  
^These are placed in annotations to configure the connection to your bucket


Inference Service
--------------------------------------------
The inference service manifest is crucial for kserve to spin up pods with the appropriate resources. 
Key configurations to focus on include:

minReplicas/scaleTarget and  autoscaling.knative.dev/target
--------------------------------------------------------------------------------------
When spinning multiple replicas change these to the number of replicas you want. 
The behaviour may change from version to version, hence to ensure that there is 2, I recommend just setting all 3 to 2 
In nebula, the minReplicas was the parameter that scaled my pods to 2
There is a short delay between multiple pods spin ups (less than 1 minute)
The same trained model resource will also be applied to both pods 

nvidia.com/gpu
--------------------------------------------------------------------------------------
This resource is declared in the inference serivce which will dictate how many gpus to reserve per pod. 
If set to 1 and the inference service is scaled to 2, it would mean that kserve will assocaiate 1 gpu for each instance 
if available



Serving Runtime
--------------------------------------------

Next, for the serving runtime, there have been significant changes, particularly in the `env` section. This new iteration of the deployment introduces 3 new environment variables.

NCCL Related
--------------------------------------------------------------

NCCL is a tool used by Triton to enable split GPU processing and model loading, allowing the model to be distributed across multiple GPUs. 
There are two types of NCCL communications: P2P (Peer-to-Peer) and Non-P2P.

- P2P is only available for certain GPUs with direct GPU-to-GPU connections. 
  (The A100 has been listed to support this capability, but it is not confirmed if it works properly.)
- RTX is not listed as supporting P2P and therefore defaults to Non-P2P.

NCCL_DEBUG
- name: NCCL_DEBUG  
- value: TRACE  
  - This variable helps trace NCCL debug errors.

NCCL_IGNORE_DISABLED_P2P
- name: NCCL_IGNORE_DISABLED_P2P  
- value: 1  
  - This variable instructs Triton to suppress P2P messages when P2P is unavailable.

If P2P is unavailable, Triton will use local RAM and memory as a substitute to transfer data between GPUs. 
For Non-P2P, volume and volume mounts are required to store the transferred data, so the following components has been added:

volumeMounts:
- mountPath: /dev/shm
  name: dshm

volumes:
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: 2Gi

