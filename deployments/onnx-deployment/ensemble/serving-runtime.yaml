apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: triton-image-runtime
spec:
  containers:
    - args:
        - tritonserver
        - "--model-repository=s3://minio-service.triton-inference-services.svc.cluster.local:9000/finalised-models/ensemble-resnet50" # Change Path accordingly
        - "--model-control-mode=explicit"
        - "--http-port=8080"
        - "--load-model=ensemble-model"  # change according to model name
      image: nvcr.io/nvidia/tritonserver:24.08-py3
      name: kserve-container
      ports:
        - name: http-port
          containerPort: 8080  # HTTP port for the inference server
        - name: grpc-port
          containerPort: 8001  # GRPC port for the inference server
        - name: metrics 
          containerPort: 8002  # Metrics port for the inference server
      env:
        - name: AWS_ACCESS_KEY_ID
          value: minio
        - name: AWS_SECRET_ACCESS_KEY
          value: minio123
        - name: AWS_DEFAULT_REGION
          value: us-east-1
        - name: HF_HOME
          value: /opt/tritonserver/caches # Path to the writable cache (For model storage)
        - name: HOME
          value: /opt/tritonserver/caches # Path to the writable cache (For model storage)
  multiModel: false
  protocolVersions:
    - v2
    - grpc-v2
  supportedModelFormats:
    - name: triton
      version: latest